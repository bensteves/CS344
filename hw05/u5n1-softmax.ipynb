{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ben Steves, CS344, 2-16-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax, part 1\n",
    "\n",
    "Task: practice using the `softmax` function.\n",
    "\n",
    "**Why**: The softmax is a building block that is used throughout machine learning, statistics, data modeling, and even statistical physics. This activity is designed to get comfortable with how it works at a high and low level.\n",
    "\n",
    "**Note**: Although \"softmax\" is the conventional name in machine learning, you may also see it called \"soft *arg* max\". The [Wikipedia article](https://en.wikipedia.org/w/index.php?title=Softmax_function&oldid=1065998663) has a good explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function defines `softmax` by using PyTorch built-in functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_torch(x):\n",
    "    return torch.softmax(x, axis=0) #computes coftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on an example tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tensor([1., 2., 3.])  #the numbers\n",
    "softmax_torch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by playing with the interactive widget below. Describe the outputs when:\n",
    "\n",
    "    1. All of the inputs are the same.\n",
    "    2. One input is much bigger than the others.\n",
    "    3. One input is much smaller than the others.\n",
    "\n",
    "Finally, describe the input that gives the largest possible value for output 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17cdfa431f049d7b038f29ab20b0119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='x0', max=2.0, min=-2.0), FloatSlider(value=0.0, descâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = 2.0\n",
    "@widgets.interact(x0=(-r, r), x1=(-r, r), x2=(-r, r))\n",
    "def show_softmax(x0, x1, x2):\n",
    "    x = tensor([x0, x1, x2])\n",
    "    xs = softmax_torch(x)\n",
    "    plt.barh([2, 1, 0], xs)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.yticks([2, 1, 0], ['output 0', 'output 1', 'output 2'])\n",
    "    plt.ylabel(\"softmax(x)\")\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. When inputs are the same, the probability output is also the same. Each of the three probabilities are 0.33 (repeating).\n",
    "\n",
    "B. When one input is much bigger, that input has the highest probability.\n",
    "\n",
    "C. When one input is much smaller, it depends on what the other two values are, but the one input is almost always much smaller than the other two.\n",
    "\n",
    "For output 1, softmax of x1 needs to be 2.0, and softmax of x0 and x2 need to be -2.0, based on the sliders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fill in the following function to implement softmax yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(xx):\n",
    "    # Exponentiate x so all numbers are positive.\n",
    "    expos = xx.exp()\n",
    "    assert expos.min() >= 0\n",
    "    # Normalize (divide by the sum).\n",
    "    return expos / sum(expos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate `softmax(x)` and verify that it is close to the `softmax_torch(x)` you evaluated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluate `softmax_torch(__)` for each of the following expressions. Observe how each output relates to `softmax_torch(x)`.\n",
    "\n",
    "- `x + 1`\n",
    "- `x - 100`\n",
    "- `x - x.max()`\n",
    "- `x * 0.5`\n",
    "- `x * 3.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x+1: tensor([0.0900, 0.2447, 0.6652]) \n",
      "x-100: tensor([0.0900, 0.2447, 0.6652]) \n",
      "x-x.max: tensor([0.0900, 0.2447, 0.6652]) \n",
      "x*0.5: tensor([0.1863, 0.3072, 0.5065]) \n",
      "x*3.0: tensor([0.0024, 0.0473, 0.9503])\n"
     ]
    }
   ],
   "source": [
    "q4_1 = softmax_torch(x+1)  #no changes\n",
    "q4_2 = softmax_torch(x-100) # no changes\n",
    "q4_3 = softmax_torch(x-x.max()) #no changes\n",
    "q4_4 = softmax_torch(x*0.5) #softmax does change, and goes up slightly for the first two and down for the last one\n",
    "q4_5 = softmax_torch(x*3.0) #softmax changes, but now a very high probability for last proportion\n",
    "print(\"x+1: %s \\nx-100: %s \\nx-x.max: %s \\nx*0.5: %s \\nx*3.0: %s\" \n",
    "      %(q4_1, q4_2, q4_3, q4_4, q4_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. *Numerical issues*. Assign `x2 = 50 * x`. Try `softmax(x2)` and observe that the result includes the dreaded `nan` -- \"not a number\". Something went wrong. **Evaluate the first mathematical operation in `softmax`** for this particularly problematic input. You should see another kind of abnormal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., nan, nan])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = 50 * x\n",
    "softmax(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.1847e+21,        inf,        inf])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.exp() #same as e^x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. *Fixing numerical issues*. Now try `softmax(x2 - 150.0)`. Observe that you now get valid numbers. Also observe how the constant we subtracted relates to the value of `x2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x2-150.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Copy your `softmax` implementation to a new function, `softmax_stable`, and change it so that it subtracts `xx.max()` before exponentiating. (Don't use any in-place operations.) Verify that `softmax_stable(x2)` now works, and obtains the same result as `softmax_torch(x2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_stable(xx):\n",
    "    # Exponentiate x so all numbers are positive.\n",
    "    expos = (xx - xx.max()).exp() #can make numbers passed to function smaller if they are too big\n",
    "    assert expos.min() >= 0\n",
    "    # Normalize (divide by the sum).\n",
    "    return expos / sum(expos)\n",
    "\n",
    "softmax_stable(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_torch(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following situation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., -1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = tensor([1., 0.,])\n",
    "x3 = x2 - 1\n",
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4 = x2 * 2\n",
    "x4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Are `softmax(x2)` and `softmax(x3)` the same or different? How could you tell without having to evaluate them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax(x2) and softmax(x3) return the same results. Subtracting values in softmax does not make a differece to the proportions because though the denominator increases or decreases, so does the numerator, and at the same rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Are `softmax(x2)` and `softmax(x4)` the same or different? How could you tell without having to evaluate them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax(x2) and x4 are different. For example, multiplying x2 by two increases x4[0] to 2, and of course x4[1] is still 0. The actual distance between the two numbers changes and now there is a larger weight to x4[1], so the softmax calculation will result in a proportion that is a little larger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain why `softmax(x2)` failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you multiply each value in the tensor by 50, and then you do e^50, e^100, and e^150 in the first part of softmax computation, which are massive numbers and probably too large to be able to store. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use your observations in \\#1-2 above to explain why `softmax_stable` still gives the correct answer even though we changed the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers inserted into softmax prior to softmax_stable were too large for python to handle. Because subtracting does not affect the proportion, we subtracted our xx value to shrink the values before exponentiating. It doesn't matter if its e^0/e^1+e^0 or e^63/e^64+e^63, they will give the same proportions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain why `softmax_stable` doesn't give us infinity or Not A Number anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going off of the previous answer, the numbers are not too large for python to handle now because we are subtracting them by the maximum numerical value in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension *optional*\n",
    "\n",
    "Try to prove your observation in Analysis \\#1 by symbolically simplifying the expression `softmax(logits + c)` and seeing if you can get `softmax(logits)`. Remember that `softmax(x) = exp(x) / exp(x).sum()` and `exp(a + b) = exp(a)exp(b)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
